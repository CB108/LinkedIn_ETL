{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some basic libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import math\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "#Web scraping imports \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "\n",
    "# For Viz\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING ROUTINE FOR EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This stays the same \n",
    "url_end1 = '&start='\n",
    "\n",
    "# ADD a += 25 to each loop \n",
    "url_end2 = 25\n",
    "\n",
    "# Feed in number of pages in search\n",
    "times_run = 0\n",
    "\n",
    "while times_run in range(5):\n",
    "    # Read_in the url \n",
    "    r = requests.get(f\"https://www.linkedin.com/jobs/search/?f_TPR=r86400&geoId=103644278&keywords=data%20analyst%20remote{url_end1}{url_end2}\")\n",
    "    # Convert to a Beautiful Soup Object \n",
    "    soup = bs(r.content)\n",
    "    # Create Variable for just the body\n",
    "    body = soup.body\n",
    "        \n",
    "    # ul class=\"jobs-search__results-list\"\n",
    "    jobs_container = body.find_all('ul', class_='jobs-search__results-list')\n",
    "        \n",
    "    # Creating a list of each li (job listing) within the jobs_container obj\n",
    "    for x in jobs_container:\n",
    "        the_li = x.find_all('li')\n",
    "            \n",
    "    # THIS grabs the a tags that contain the links to each job  \n",
    "    job_num_list = []\n",
    "\n",
    "    for x in the_li:\n",
    "        job_num = x.find(attrs={\"data-id\" : \"\"})\n",
    "        job_num_list.append(job_num)\n",
    "            \n",
    "    # THIS grabs the LINKS themselves and puts them into a list\n",
    "    link_list = []\n",
    "\n",
    "    for i in job_num_list:\n",
    "        linky = i.get('href')\n",
    "        link_list.append(linky)\n",
    "            \n",
    "    # THIS grabs the html for each job posting that we now need \n",
    "    for each_link in link_list:\n",
    "        one_link = requests.get(each_link)\n",
    "        soup_one_link = bs(one_link.content)\n",
    "        body_one_link = soup_one_link.body\n",
    "        # Then we pair down to the h3 tag where the info is \n",
    "        pair_down_1 = body_one_link.find_all('h3',class_=\"topcard__flavor-row\")\n",
    "            \n",
    "        # Creates a list to hold each jobs info \n",
    "        each_job = []\n",
    "\n",
    "        # Grabs company name\n",
    "        company_name = body_one_link.find('a', class_=\"sub-nav-cta__optional-url\")\n",
    "        if company_name:\n",
    "            each_job.append(company_name.text)\n",
    "\n",
    "        # Grabs job title \n",
    "        job_title = body_one_link.find('h3', class_=\"sub-nav-cta__header\")\n",
    "        if job_title:\n",
    "            each_job.append(job_title.text)\n",
    "            \n",
    "        # Grabs the number of applicants and time since posted \n",
    "        for x in pair_down_1:\n",
    "\n",
    "            time_ago = x.find('span', class_=\"topcard__flavor--metadata posted-time-ago__text posted-time-ago__text--new\")\n",
    "            num_applicants = x.find(class_=\"topcard__flavor--metadata topcard__flavor--bullet num-applicants__caption\")\n",
    "            be_among = x.find(class_='num-applicants__caption')\n",
    "\n",
    "\n",
    "            if time_ago:\n",
    "                each_job.append(time_ago.text)\n",
    "            if num_applicants:\n",
    "                each_job.append(num_applicants.text)\n",
    "            if be_among:\n",
    "                each_job.append(be_among.text)\n",
    "                  \n",
    "        master_list_LI.append(each_job)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    times_run += 1\n",
    "    url_end2 -= 25\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating full list for date scraped\n",
    "page_one_april12 = master_list_LI.copy()\n",
    "the_rest_april12 = master_list_LI.copy()\n",
    "april12_linked = page_one_april12 + the_rest_april12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE CLEAN FOR SPLIT ENDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning \n",
    "april12_for_df = [x[:-1] if len(x) > 4 else x for x in april12_linked]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING TO LOCAL DRIVE FOR FURTHER WORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DF\n",
    "april12_DF = pd.DataFrame(april12_for_df, columns = ['company', 'job_title', 'time_elapsed', 'applicants' ])\n",
    "\n",
    "#Exporting for further work in excel \n",
    "april12_DF.to_csv('linked_april_12.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
